{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI544 Homework2 - Ravi Chandra Reddy Basireddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "- Pandas: To work with dataframes.\n",
    "- NLTK: A Natural Language Toolkit used for processing textual data.\n",
    "- RE: Regular Expressions used for handling word findings & substitutions.\n",
    "- BS4: BeautifulSoup Library is a parser that can handle HTML Tags and Links.\n",
    "- Contractions: A library to contract and de-contract Contractions.\n",
    "- String: A library to handle strings.\n",
    "- Warning: A library to handle console warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet',quiet=True)\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "\n",
    "#!pip install bs4 \n",
    "#!pip install contractions\n",
    "#!pip install nltk \n",
    "#!pip install string\n",
    "#!pip install pandas \n",
    "#!pip install warnings\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "- Read Data from a TSV file where the data is seperated using tabs.\n",
    "- we are only intrested in Star Rating and Review Body.\n",
    "- Star Rating: Rating given by the customers in the range of 1 to 5.\n",
    "- Review Body: Review given by the customers in the textual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews=pd.read_csv('data.tsv', sep='\\t',usecols=['star_rating','review_body'],low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings\n",
    "- Already completed at the reading data step.\n",
    "- Dropping NaN Values which has no meaning to the rating.\n",
    "- Dropping Duplicates which are repeated.\n",
    "- Printing out the first five values to know what kind of data is in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                        review_body\n",
       "0           5  so beautiful even tho clearly not high end ......\n",
       "1           5  Great product.. I got this set for my mother, ...\n",
       "2           5  Exactly as pictured and my daughter's friend l...\n",
       "3           5  Love it. Fits great. Super comfortable and nea...\n",
       "4           5  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews=amazon_reviews.dropna()\n",
    "amazon_reviews=amazon_reviews.drop_duplicates()\n",
    "amazon_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We select 20000 reviews randomly from each rating class.\n",
    "- filtering out the data with respective labels.\n",
    "- sampling 20k reviews from each class.\n",
    "- Combining all the data from differnt classes to create a vector of Dimension (100000,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_one=amazon_reviews[amazon_reviews.star_rating=='1']\n",
    "star_one=star_one.sample(n=20000)\n",
    "star_two=amazon_reviews[amazon_reviews.star_rating=='2']\n",
    "star_two=star_two.sample(n=20000)\n",
    "star_three=amazon_reviews[amazon_reviews.star_rating=='3']\n",
    "star_three=star_three.sample(n=20000)\n",
    "star_four=amazon_reviews[amazon_reviews.star_rating=='4']\n",
    "star_four=star_four.sample(n=20000)\n",
    "star_five=amazon_reviews[amazon_reviews.star_rating=='5']\n",
    "star_five=star_five.sample(n=20000)\n",
    "sampled_reviews=pd.concat([star_one,star_two,star_three,star_four,star_five],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- Cleaning the data inorder to make the models better, as better data will always result better Prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "1. Removing URL.\n",
    "2. Removing HTML Tags.\n",
    "3. Removing All the characters except for A-Z&a-z.\n",
    "4. Removing any html text left with BeautifulSoup Library.\n",
    "5. Removing Contractions.\n",
    "6. Removing Punctuation.\n",
    "7. Removing extra Spaces.\n",
    "8. Converting the text to lowecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(review):\n",
    "    return ''.join([words for words in review if words not in string.punctuation ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):    \n",
    "    review = re.sub(r\"http\\S+\", \"\", review)\n",
    "    review = re.sub('<.*?>+', '', review)\n",
    "    review = re.sub('[^A-Za-z]+', ' ', review)\n",
    "    review = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    review = contractions.fix(review)\n",
    "    review = remove_punctuation(review)\n",
    "    review = re.sub(\"\\S*\\d\\S*\", \"\", review).strip()\n",
    "    review = review.lower()\n",
    "    return review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Average Length of Reviews by Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_count(sampled_reviews):\n",
    "    number_of_sentences=len(sampled_reviews)\n",
    "    return sum(map(len,sampled_reviews))/number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of the reviews Before and After Cleaning 197.64722 , 189.79002\n"
     ]
    }
   ],
   "source": [
    "beforeCleaning=average_count(sampled_reviews['review_body'])\n",
    "sampled_reviews['review_body']=sampled_reviews['review_body'].apply(lambda review:clean_review(review))\n",
    "afterCleaning=average_count(sampled_reviews['review_body'])\n",
    "print(\"Average character length of the reviews Before and After Cleaning\",beforeCleaning,',',afterCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences=[]\n",
    "for sentence in sampled_reviews['review_body'].values:\n",
    "    list_of_sentences.append(sentence.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377322435379028),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134939193726),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive = ['king', 'woman'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.7409728765487671),\n",
       " ('superb', 0.7062716484069824),\n",
       " ('exceptional', 0.681470513343811),\n",
       " ('fantastic', 0.6802847385406494),\n",
       " ('good', 0.644292950630188),\n",
       " ('great', 0.6124600768089294),\n",
       " ('Excellent', 0.6091997623443604),\n",
       " ('impeccable', 0.5980966687202454),\n",
       " ('exemplary', 0.5959650278091431),\n",
       " ('marvelous', 0.5829284191131592)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive = ['excellent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('month', 0.34959733486175537),\n",
       " ('year', 0.3340516984462738),\n",
       " ('months', 0.3266761004924774),\n",
       " ('week', 0.3075323700904846),\n",
       " ('summer', 0.30571261048316956),\n",
       " ('years', 0.3044481873512268),\n",
       " ('moment', 0.3007998466491699),\n",
       " ('season', 0.3001079261302948),\n",
       " ('opportunity', 0.29813849925994873),\n",
       " ('weeks', 0.2971378564834595)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar(positive = ['time'],negative=['clock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_amazon_model=Word2Vec(list_of_sentences,min_count=10,vector_size=300,window=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lifetime', 0.5438417196273804),\n",
       " ('west', 0.5429157614707947),\n",
       " ('world', 0.5113186836242676),\n",
       " ('co', 0.5072171688079834),\n",
       " ('responsible', 0.4986431300640106),\n",
       " ('teenagers', 0.4971093237400055),\n",
       " ('future', 0.4936424791812897),\n",
       " ('purposes', 0.4877610206604004),\n",
       " ('copy', 0.4861401617527008),\n",
       " ('pompeii', 0.4840492308139801)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_amazon_model.wv.most_similar(positive = ['king', 'woman'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exceptional', 0.8293458223342896),\n",
       " ('outstanding', 0.7952795028686523),\n",
       " ('superb', 0.7474648356437683),\n",
       " ('inferior', 0.709937334060669),\n",
       " ('amazing', 0.676922619342804),\n",
       " ('lacking', 0.6761174201965332),\n",
       " ('decent', 0.6621876955032349),\n",
       " ('poor', 0.6587395668029785),\n",
       " ('acceptable', 0.6580824851989746),\n",
       " ('fantastic', 0.6437696814537048)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_amazon_model.wv.most_similar(positive = ['excellent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('day', 0.5510172247886658),\n",
       " ('once', 0.3180447816848755),\n",
       " ('spent', 0.30497655272483826),\n",
       " ('waiting', 0.2866770327091217),\n",
       " ('chance', 0.2708539664745331),\n",
       " ('morning', 0.2696778178215027),\n",
       " ('penny', 0.2651486396789551),\n",
       " ('twice', 0.262528657913208),\n",
       " ('week', 0.26093748211860657),\n",
       " ('compliments', 0.25867024064064026)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_amazon_model.wv.most_similar(positive = ['time'],negative=['clock'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Word2Vec Model performs better than amazon word2vec model because it is trained with billions of features,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_wv=[]\n",
    "for sentence in list_of_sentences:\n",
    "    sentence_vectors=np.zeros(300)\n",
    "    number_of_words=0\n",
    "    for words in sentence:\n",
    "        try:\n",
    "            vector=word2vec_model[words]\n",
    "            sentence_vectors=np.add(sentence_vectors,vector)\n",
    "            number_of_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    if number_of_words != 0:\n",
    "        sentence_vectors/=number_of_words\n",
    "    average_wv.append(sentence_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction\n",
    "- Converting Reviews to Count Vectors using a concept known as TF-IDF.\n",
    "- It is the relation between Term Frequency and Inverse Document Frequency.\n",
    "- Term Frequency is the frequency of the word in a corpus.\n",
    "- Inverse Document Frequecy is the Frequency of a Word in that particular Document.\n",
    "- TF - IDF tells about the Frequency of a Word in that Particular Document With Respect to the Entire Corpus.\n",
    "- N-Grams are combination of words in that particular document. Bi-gram Example (really-appreciate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,3))\n",
    "final_tf_idf=tf_idf_vect.fit_transform(sampled_reviews['review_body'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "- We split the data in the split of 80:20 which is 80% of the for Training and 20% of the Data for Testing.\n",
    "- We use train test split in order to train the model and test performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "xtrain, xtest, ytrain, ytest = train_test_split(average_wv,sampled_reviews['star_rating'] , test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "xtrain_tfidf, xtest_tfidf, ytrain_tfidf, ytest_tfidf = train_test_split(final_tf_idf, sampled_reviews['star_rating'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=np.array(xtrain)\n",
    "xtest=np.array(xtest)\n",
    "ytrain=np.array(ytrain).astype(np.float16)\n",
    "ytest=np.array(ytest).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "A Function that gives you information on Accuracy, Precision, Recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "\n",
    "def metrics(prediction, actual): \n",
    "    print('\\nAccuracy:', accuracy_score(actual, prediction))\n",
    "    print('\\nclassification_report\\n')\n",
    "    print(classification_report(actual, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "- Perceptron is a two class classification Model.\n",
    "- It uses a concept of Neuron, which has an activation function, which activates only when crossing a certain threshold.\n",
    "- We used random_state to randomize the data.\n",
    "- We used n_jobs to Run Parallel on All Cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.4405\n",
      "\n",
      "classification_report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.60      0.42      0.50      3929\n",
      "         2.0       0.33      0.53      0.40      4040\n",
      "         3.0       0.35      0.31      0.33      4010\n",
      "         4.0       0.44      0.23      0.30      3975\n",
      "         5.0       0.56      0.71      0.63      4046\n",
      "\n",
      "    accuracy                           0.44     20000\n",
      "   macro avg       0.46      0.44      0.43     20000\n",
      "weighted avg       0.46      0.44      0.43     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptronModel = Perceptron(random_state=0,n_jobs=-1)\n",
    "perceptronModel.fit(xtrain, ytrain)\n",
    "predictions=perceptronModel.predict(xtest)  \n",
    "metrics(predictions, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.49465\n",
      "\n",
      "classification_report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.66      0.60      3990\n",
      "           2       0.41      0.27      0.32      4020\n",
      "           3       0.40      0.41      0.40      4042\n",
      "           4       0.43      0.44      0.44      3958\n",
      "           5       0.62      0.70      0.66      3990\n",
      "\n",
      "    accuracy                           0.49     20000\n",
      "   macro avg       0.48      0.50      0.49     20000\n",
      "weighted avg       0.48      0.49      0.49     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptronModel = Perceptron(random_state=0,n_jobs=-1)\n",
    "perceptronModel.fit(xtrain_tfidf, ytrain_tfidf)\n",
    "predictions=perceptronModel.predict(xtest_tfidf)  \n",
    "metrics(predictions, ytest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Perceptron with TFIDF performs better as we are taking Average Word2Vec which losses lot of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "- SVM uses a concept of boundary, which helps it to detect and avoid outliers.\n",
    "- SVM have differnt form of Kernel: Linear, Poly and more, which can be used fir different firms of data.\n",
    "- We used C=0.1 which is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.4894\n",
      "\n",
      "classification_report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.50      0.73      0.59      3929\n",
      "         2.0       0.41      0.27      0.32      4040\n",
      "         3.0       0.41      0.37      0.39      4010\n",
      "         4.0       0.45      0.32      0.37      3975\n",
      "         5.0       0.59      0.76      0.67      4046\n",
      "\n",
      "    accuracy                           0.49     20000\n",
      "   macro avg       0.47      0.49      0.47     20000\n",
      "weighted avg       0.47      0.49      0.47     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM = LinearSVC(C=0.1)\n",
    "SVM.fit(xtrain, ytrain)\n",
    "predictions=SVM.predict(xtest)\n",
    "metrics(predictions, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.54945\n",
      "\n",
      "classification_report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.73      0.64      3990\n",
      "           2       0.45      0.33      0.38      4020\n",
      "           3       0.48      0.43      0.45      4042\n",
      "           4       0.53      0.44      0.48      3958\n",
      "           5       0.65      0.82      0.73      3990\n",
      "\n",
      "    accuracy                           0.55     20000\n",
      "   macro avg       0.54      0.55      0.54     20000\n",
      "weighted avg       0.54      0.55      0.54     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "SVM = LinearSVC(C=0.1)\n",
    "SVM.fit(xtrain_tfidf, ytrain_tfidf)\n",
    "predictions=SVM.predict(xtest_tfidf)\n",
    "metrics(predictions, ytest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : SVM with TFIDF performs better as we are taking Average Word2Vec which losses lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Pytorch\n",
    "- TensorDataset for creating dataset from traing and test data\n",
    "- Dataloader for loading tensorDataset\n",
    "- nn module for creating custom Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out whether CPU or GPU in Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.has_mps:\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device =\"cpu\" \n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    count=0\n",
    "    for data, target in dataloader:\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        for x in range(len(predicted)):\n",
    "            if(target[x]-1==predicted[x]):\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_hidden(model, dataloader):\n",
    "    count=0\n",
    "    hidden_state = model.init_hidden(40)\n",
    "    for data, target in dataloader:\n",
    "        outputs,_ = model(data,h)\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        for x in range(len(predicted)):\n",
    "            if(target[x]-1==predicted[x]):\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TesnorDataset and DataLoader\n",
    "- TensorDataset for creating dataset from traing and test data\n",
    "- Dataloader for loading tensorDataset\n",
    "- we are using batchsize of 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def creating_loading_tensor(X,y,batch):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X,y, test_size = 0.2)\n",
    "    xtrain=np.array(xtrain)\n",
    "    xtest=np.array(xtest)\n",
    "    ytrain=np.array(ytrain).astype(np.float16)\n",
    "    ytest=np.array(ytest).astype(np.float16)\n",
    "    test_data = TensorDataset(torch.FloatTensor(xtest),  torch.LongTensor(ytest))\n",
    "    train_data = TensorDataset(torch.FloatTensor(xtrain),  torch.LongTensor(ytrain))\n",
    "    batch_size = batch\n",
    "    train_loader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle = True, batch_size = batch_size)\n",
    "    return train_loader,test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network\n",
    "- Input Layer : 300\n",
    "- Hidden Layer 1 : 50\n",
    "- Hidden Layer 2 : 10\n",
    "- Output Layer : 5\n",
    "- We are using RELU as Activation Function\n",
    "- We are using CrossEntropy as loss Function\n",
    "- We are using SGD for Optimzation\n",
    "\n",
    "- Reference : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN Using Average Word2Vve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader=creating_loading_tensor(average_wv,sampled_reviews['star_rating'],batch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(1*300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.609100\n",
      "Epoch: 2 \tTraining Loss: 1.604974\n",
      "Epoch: 3 \tTraining Loss: 1.594845\n",
      "Epoch: 4 \tTraining Loss: 1.554497\n",
      "Epoch: 5 \tTraining Loss: 1.455629\n",
      "Epoch: 6 \tTraining Loss: 1.381124\n",
      "Epoch: 7 \tTraining Loss: 1.344667\n",
      "Epoch: 8 \tTraining Loss: 1.325516\n",
      "Epoch: 9 \tTraining Loss: 1.311536\n",
      "Epoch: 10 \tTraining Loss: 1.297184\n",
      "Epoch: 11 \tTraining Loss: 1.288385\n",
      "Epoch: 12 \tTraining Loss: 1.280477\n",
      "Epoch: 13 \tTraining Loss: 1.271495\n",
      "Epoch: 14 \tTraining Loss: 1.265411\n",
      "Epoch: 15 \tTraining Loss: 1.257448\n",
      "Epoch: 16 \tTraining Loss: 1.251293\n",
      "Epoch: 17 \tTraining Loss: 1.245571\n",
      "Epoch: 18 \tTraining Loss: 1.240104\n",
      "Epoch: 19 \tTraining Loss: 1.236047\n",
      "Epoch: 20 \tTraining Loss: 1.232546\n",
      "Epoch: 21 \tTraining Loss: 1.228585\n",
      "Epoch: 22 \tTraining Loss: 1.226145\n",
      "Epoch: 23 \tTraining Loss: 1.224111\n",
      "Epoch: 24 \tTraining Loss: 1.221565\n",
      "Epoch: 25 \tTraining Loss: 1.218959\n",
      "Epoch: 26 \tTraining Loss: 1.215534\n",
      "Epoch: 27 \tTraining Loss: 1.215866\n",
      "Epoch: 28 \tTraining Loss: 1.214642\n",
      "Epoch: 29 \tTraining Loss: 1.211396\n",
      "Epoch: 30 \tTraining Loss: 1.210918\n",
      "Epoch: 31 \tTraining Loss: 1.209289\n",
      "Epoch: 32 \tTraining Loss: 1.206202\n",
      "Epoch: 33 \tTraining Loss: 1.205123\n",
      "Epoch: 34 \tTraining Loss: 1.204540\n",
      "Epoch: 35 \tTraining Loss: 1.204996\n",
      "Epoch: 36 \tTraining Loss: 1.201968\n",
      "Epoch: 37 \tTraining Loss: 1.203151\n",
      "Epoch: 38 \tTraining Loss: 1.202018\n",
      "Epoch: 39 \tTraining Loss: 1.200992\n",
      "Epoch: 40 \tTraining Loss: 1.199943\n",
      "Epoch: 41 \tTraining Loss: 1.196884\n",
      "Epoch: 42 \tTraining Loss: 1.197687\n",
      "Epoch: 43 \tTraining Loss: 1.196309\n",
      "Epoch: 44 \tTraining Loss: 1.195645\n",
      "Epoch: 45 \tTraining Loss: 1.196932\n",
      "Epoch: 46 \tTraining Loss: 1.193906\n",
      "Epoch: 47 \tTraining Loss: 1.195524\n",
      "Epoch: 48 \tTraining Loss: 1.193642\n",
      "Epoch: 49 \tTraining Loss: 1.192101\n",
      "Epoch: 50 \tTraining Loss: 1.189703\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    model.train() \n",
    "    for data,target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(train_loader.dataset)  \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4751\n"
     ]
    }
   ],
   "source": [
    "count = predict(model,test_loader)\n",
    "print(count/len(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FNN Accuracy with Average Word2Vec is 47.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNN with the first 10 Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_wv=[]\n",
    "for sentence in list_of_sentences:\n",
    "    sentence_vectors=np.zeros(300)\n",
    "    sentence=sentence[:10]\n",
    "    count=0\n",
    "    for words in sentence:\n",
    "        try:\n",
    "            vector=word2vec_model[words]\n",
    "            sentence_vectors+=vector\n",
    "            number_of_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    ten_wv.append(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader=creating_loading_tensor(ten_wv,sampled_reviews['star_rating'],batch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.454457\n",
      "Epoch: 2 \tTraining Loss: 1.411800\n",
      "Epoch: 3 \tTraining Loss: 1.391481\n",
      "Epoch: 4 \tTraining Loss: 1.378038\n",
      "Epoch: 5 \tTraining Loss: 1.368624\n",
      "Epoch: 6 \tTraining Loss: 1.359985\n",
      "Epoch: 7 \tTraining Loss: 1.355694\n",
      "Epoch: 8 \tTraining Loss: 1.347346\n",
      "Epoch: 9 \tTraining Loss: 1.344286\n",
      "Epoch: 10 \tTraining Loss: 1.339169\n",
      "Epoch: 11 \tTraining Loss: 1.332864\n",
      "Epoch: 12 \tTraining Loss: 1.330800\n",
      "Epoch: 13 \tTraining Loss: 1.325207\n",
      "Epoch: 14 \tTraining Loss: 1.322960\n",
      "Epoch: 15 \tTraining Loss: 1.319139\n",
      "Epoch: 16 \tTraining Loss: 1.317790\n",
      "Epoch: 17 \tTraining Loss: 1.311468\n",
      "Epoch: 18 \tTraining Loss: 1.311271\n",
      "Epoch: 19 \tTraining Loss: 1.309242\n",
      "Epoch: 20 \tTraining Loss: 1.306897\n",
      "Epoch: 21 \tTraining Loss: 1.305781\n",
      "Epoch: 22 \tTraining Loss: 1.304713\n",
      "Epoch: 23 \tTraining Loss: 1.300655\n",
      "Epoch: 24 \tTraining Loss: 1.299550\n",
      "Epoch: 25 \tTraining Loss: 1.296511\n",
      "Epoch: 26 \tTraining Loss: 1.294990\n",
      "Epoch: 27 \tTraining Loss: 1.293665\n",
      "Epoch: 28 \tTraining Loss: 1.290324\n",
      "Epoch: 29 \tTraining Loss: 1.290615\n",
      "Epoch: 30 \tTraining Loss: 1.289189\n",
      "Epoch: 31 \tTraining Loss: 1.288014\n",
      "Epoch: 32 \tTraining Loss: 1.285990\n",
      "Epoch: 33 \tTraining Loss: 1.285855\n",
      "Epoch: 34 \tTraining Loss: 1.285038\n",
      "Epoch: 35 \tTraining Loss: 1.280020\n",
      "Epoch: 36 \tTraining Loss: 1.281092\n",
      "Epoch: 37 \tTraining Loss: 1.280250\n",
      "Epoch: 38 \tTraining Loss: 1.275528\n",
      "Epoch: 39 \tTraining Loss: 1.277442\n",
      "Epoch: 40 \tTraining Loss: 1.275224\n",
      "Epoch: 41 \tTraining Loss: 1.274126\n",
      "Epoch: 42 \tTraining Loss: 1.274759\n",
      "Epoch: 43 \tTraining Loss: 1.271891\n",
      "Epoch: 44 \tTraining Loss: 1.270694\n",
      "Epoch: 45 \tTraining Loss: 1.270272\n",
      "Epoch: 46 \tTraining Loss: 1.271610\n",
      "Epoch: 47 \tTraining Loss: 1.270159\n",
      "Epoch: 48 \tTraining Loss: 1.266529\n",
      "Epoch: 49 \tTraining Loss: 1.267950\n",
      "Epoch: 50 \tTraining Loss: 1.265401\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0    \n",
    "    model.train() # prep model for training\n",
    "    for data,target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)    \n",
    "    train_loss = train_loss/len(train_loader.dataset)    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41185\n"
     ]
    }
   ],
   "source": [
    "count = predict(model,test_loader)\n",
    "print(count/len(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FNN Accuracy with 10 Word2Vec is 41.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Feed Forward Network with AverageWord2Vec performs better as we are cosidering only 10 words in the 10 Word2Vec Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reccurent Neural Network\n",
    "- Input Layer : 300\n",
    "- Hidden Layer 1 : 20\n",
    "- Output Layer : 5\n",
    "- We are using Softmax as Activation Function\n",
    "- We are using CrossEntropy as loss Function\n",
    "- We are using SGD for Optimzation\n",
    "\n",
    "- Reference : https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,batch):\n",
    "        return torch.zeros(batch,self.hidden_size,requires_grad=False)\n",
    "\n",
    "n_hidden = 20\n",
    "model = RNN(300, n_hidden, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_wv=[]\n",
    "for sentence in list_of_sentences:\n",
    "    sentence_vectors=np.zeros(300)\n",
    "    number_of_words=0\n",
    "    sentence=sentence[:20]\n",
    "    while len(sentence) < 20:\n",
    "        sentence.append(0)\n",
    "    \n",
    "    for words in sentence:\n",
    "        try:\n",
    "            vector=word2vec_model[words]\n",
    "            sentence_vectors=np.add(sentence_vectors,vector)\n",
    "            number_of_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    twenty_wv.append(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader=creating_loading_tensor(twenty_wv,sampled_reviews['star_rating'],batch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.364559\n",
      "Epoch: 2 \tTraining Loss: 1.304589\n",
      "Epoch: 3 \tTraining Loss: 1.293882\n",
      "Epoch: 4 \tTraining Loss: 1.288379\n",
      "Epoch: 5 \tTraining Loss: 1.284857\n",
      "Epoch: 6 \tTraining Loss: 1.282694\n",
      "Epoch: 7 \tTraining Loss: 1.281443\n",
      "Epoch: 8 \tTraining Loss: 1.280229\n",
      "Epoch: 9 \tTraining Loss: 1.278944\n",
      "Epoch: 10 \tTraining Loss: 1.279484\n",
      "Epoch: 11 \tTraining Loss: 1.278861\n",
      "Epoch: 12 \tTraining Loss: 1.279209\n",
      "Epoch: 13 \tTraining Loss: 1.277360\n",
      "Epoch: 14 \tTraining Loss: 1.277551\n",
      "Epoch: 15 \tTraining Loss: 1.278160\n",
      "Epoch: 16 \tTraining Loss: 1.277251\n",
      "Epoch: 17 \tTraining Loss: 1.277375\n",
      "Epoch: 18 \tTraining Loss: 1.276739\n",
      "Epoch: 19 \tTraining Loss: 1.276298\n",
      "Epoch: 20 \tTraining Loss: 1.276457\n",
      "Epoch: 21 \tTraining Loss: 1.276681\n",
      "Epoch: 22 \tTraining Loss: 1.276729\n",
      "Epoch: 23 \tTraining Loss: 1.276244\n",
      "Epoch: 24 \tTraining Loss: 1.275920\n",
      "Epoch: 25 \tTraining Loss: 1.276702\n",
      "Epoch: 26 \tTraining Loss: 1.275511\n",
      "Epoch: 27 \tTraining Loss: 1.276304\n",
      "Epoch: 28 \tTraining Loss: 1.275553\n",
      "Epoch: 29 \tTraining Loss: 1.276954\n",
      "Epoch: 30 \tTraining Loss: 1.275532\n",
      "Epoch: 31 \tTraining Loss: 1.275807\n",
      "Epoch: 32 \tTraining Loss: 1.276120\n",
      "Epoch: 33 \tTraining Loss: 1.276136\n",
      "Epoch: 34 \tTraining Loss: 1.276343\n",
      "Epoch: 35 \tTraining Loss: 1.275836\n",
      "Epoch: 36 \tTraining Loss: 1.276034\n",
      "Epoch: 37 \tTraining Loss: 1.276991\n",
      "Epoch: 38 \tTraining Loss: 1.276021\n",
      "Epoch: 39 \tTraining Loss: 1.276114\n",
      "Epoch: 40 \tTraining Loss: 1.276023\n",
      "Epoch: 41 \tTraining Loss: 1.275366\n",
      "Epoch: 42 \tTraining Loss: 1.276135\n",
      "Epoch: 43 \tTraining Loss: 1.276083\n",
      "Epoch: 44 \tTraining Loss: 1.275954\n",
      "Epoch: 45 \tTraining Loss: 1.276924\n",
      "Epoch: 46 \tTraining Loss: 1.276170\n",
      "Epoch: 47 \tTraining Loss: 1.276346\n",
      "Epoch: 48 \tTraining Loss: 1.275218\n",
      "Epoch: 49 \tTraining Loss: 1.276551\n",
      "Epoch: 50 \tTraining Loss: 1.276341\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    hidden_state = model.initHidden(40)\n",
    "    model.train() # prep model for training\n",
    "    for data,target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output,hidden = model(data,hidden_state)\n",
    "        loss = criterion(output, target-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    count=0\n",
    "    hidden_state = model.initHidden(40)\n",
    "    for data, target in dataloader:\n",
    "        outputs,_ = model(data,hidden_state)\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        for x in range(len(predicted)):\n",
    "            if(target[x]-1==predicted[x]):\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44785\n"
     ]
    }
   ],
   "source": [
    "count = predict(model,test_loader)\n",
    "print(count/len(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The  RNN Accuracy with 20 Word2Vec is 44.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conculusion RNN Perform Better to FNN when considering 20 Words, But its less than Average Word2Vec FNN as it omits most of the words and sometimes it suffers from Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit Cell\n",
    "- Input Layer : 300\n",
    "- Hidden Layer 1 : 20\n",
    "- Output Layer : 5\n",
    "- We are using Softmax as Activation Function\n",
    "- We are using CrossEntropy as loss Function\n",
    "- We are using SGD for Optimzation\n",
    "- GRU has 2 gates Update, Reset which controls the flow in the network\n",
    "- Reference : https://pytorch.org/docs/stable/generated/torch.nn.GRU.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1,:]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        return hidden\n",
    "\n",
    "n_hidden = 20\n",
    "model = GRUModel(300,n_hidden,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_wv=[]\n",
    "for sentence in list_of_sentences:\n",
    "    sentence_vectors=[]\n",
    "    number_of_words=0\n",
    "    sentence=sentence[:20]\n",
    "   \n",
    "    count=0\n",
    "    for words in sentence:\n",
    "        try:\n",
    "            vector=word2vec_model[words]\n",
    "            sentence_vectors.append(vector)\n",
    "            number_of_words+=1\n",
    "            count+=1\n",
    "        except:\n",
    "            pass\n",
    "    while count<20:\n",
    "        sentence_vectors.append(np.zeros(300))\n",
    "        count+=1\n",
    "    twenty_wv.append(sentence_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader=creating_loading_tensor(twenty_wv,sampled_reviews['star_rating'],batch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.610465\n",
      "Epoch: 2 \tTraining Loss: 1.608487\n",
      "Epoch: 3 \tTraining Loss: 1.607531\n",
      "Epoch: 4 \tTraining Loss: 1.606474\n",
      "Epoch: 5 \tTraining Loss: 1.605117\n",
      "Epoch: 6 \tTraining Loss: 1.603292\n",
      "Epoch: 7 \tTraining Loss: 1.600669\n",
      "Epoch: 8 \tTraining Loss: 1.595951\n",
      "Epoch: 9 \tTraining Loss: 1.568398\n",
      "Epoch: 10 \tTraining Loss: 1.419354\n",
      "Epoch: 11 \tTraining Loss: 1.373118\n",
      "Epoch: 12 \tTraining Loss: 1.349225\n",
      "Epoch: 13 \tTraining Loss: 1.335193\n",
      "Epoch: 14 \tTraining Loss: 1.324397\n",
      "Epoch: 15 \tTraining Loss: 1.316847\n",
      "Epoch: 16 \tTraining Loss: 1.308476\n",
      "Epoch: 17 \tTraining Loss: 1.301670\n",
      "Epoch: 18 \tTraining Loss: 1.295073\n",
      "Epoch: 19 \tTraining Loss: 1.286345\n",
      "Epoch: 20 \tTraining Loss: 1.277574\n",
      "Epoch: 21 \tTraining Loss: 1.267182\n",
      "Epoch: 22 \tTraining Loss: 1.255956\n",
      "Epoch: 23 \tTraining Loss: 1.249106\n",
      "Epoch: 24 \tTraining Loss: 1.242698\n",
      "Epoch: 25 \tTraining Loss: 1.236998\n",
      "Epoch: 26 \tTraining Loss: 1.231249\n",
      "Epoch: 27 \tTraining Loss: 1.228434\n",
      "Epoch: 28 \tTraining Loss: 1.223186\n",
      "Epoch: 29 \tTraining Loss: 1.219947\n",
      "Epoch: 30 \tTraining Loss: 1.215416\n",
      "Epoch: 31 \tTraining Loss: 1.211686\n",
      "Epoch: 32 \tTraining Loss: 1.208664\n",
      "Epoch: 33 \tTraining Loss: 1.206271\n",
      "Epoch: 34 \tTraining Loss: 1.203801\n",
      "Epoch: 35 \tTraining Loss: 1.199405\n",
      "Epoch: 36 \tTraining Loss: 1.197586\n",
      "Epoch: 37 \tTraining Loss: 1.195033\n",
      "Epoch: 38 \tTraining Loss: 1.192427\n",
      "Epoch: 39 \tTraining Loss: 1.190276\n",
      "Epoch: 40 \tTraining Loss: 1.187385\n",
      "Epoch: 41 \tTraining Loss: 1.185006\n",
      "Epoch: 42 \tTraining Loss: 1.184140\n",
      "Epoch: 43 \tTraining Loss: 1.181240\n",
      "Epoch: 44 \tTraining Loss: 1.179079\n",
      "Epoch: 45 \tTraining Loss: 1.177106\n",
      "Epoch: 46 \tTraining Loss: 1.175705\n",
      "Epoch: 47 \tTraining Loss: 1.173531\n",
      "Epoch: 48 \tTraining Loss: 1.171837\n",
      "Epoch: 49 \tTraining Loss: 1.170525\n",
      "Epoch: 50 \tTraining Loss: 1.168343\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    init_hidden=model.init_hidden(40)\n",
    "    model.train() # prep model for training\n",
    "    for data,target in train_loader:\n",
    "        h = init_hidden.data\n",
    "        optimizer.zero_grad()\n",
    "        output,h = model(data,h)\n",
    "        loss = criterion(output, target-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "  \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    count=0\n",
    "    hidden_state = model.init_hidden(40)\n",
    "    for data, target in dataloader:\n",
    "        outputs,_ = model(data,h)\n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        for x in range(len(predicted)):\n",
    "            if(target[x]-1==predicted[x]):\n",
    "                count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31525\n"
     ]
    }
   ],
   "source": [
    "count = predict(model,test_loader)\n",
    "print(count/len(ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GRU Accuracy with 20 Word2Vec is 40.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : The Performance of GRU is comparible to RNN but as we are considering only 20 words it doesnt help much as most of the information is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
